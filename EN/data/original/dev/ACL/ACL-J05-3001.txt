Evaluating Discourse and Dialogue Coding Schemes

Agreement statistics play an important role in the evaluation of coding schemes for discourse and dialogue.
Unfortunately there is a lack of understanding regarding appropriate agreement measures and how their results should be interpreted.
In this article we describe the role of agreement measures and argue that only chance-corrected measures that assume a common distribution of labels for all coders are suitable for measuring agreement in reliability studies.
We then provide recommendations for how reliability should be inferred from the results of agreement statistics.
Since Jean Carletta (1996) exposed computational linguists to the desirability of using chance-corrected agreement statistics to infer the reliability of data generated by applying coding schemes, there has been a general acceptance of their use within the field.
However, there are prevailing misunderstandings concerning agreement statistics and the meaning of reliability.
Investigation of new dialogue types and genres has been shown to reveal new phenomena in dialogue that are ill suited to annotation by current methods and also new annotation schemes that are qualitatively different from those commonly used in dialogue analysis.
Previously prescribed practices for evaluating coding schemes become less applicable as annotation schemes become more sophisticated.
To compensate, we need a greater understanding of reliability statistics and how they should be interpreted.
In this article we discuss the purpose of reliability testing, address certain misunderstandings, and make recommendations regarding the way in which coding schemes should be evaluated.
