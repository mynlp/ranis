Estimating the Reliability of MDP Policies : a Confidence Interval Approach .
Past approaches for using reinforcement learning to derive dialog control policies have assumed that there was enough collected data to derive a reliable policy .
In this paper we present a methodology for numerically constructing con dence intervals for the expected cumulative reward for a learned policy .
These intervals are used to ( 1 ) better assess the reliability of the expected cumulative reward , and ( 2 ) perform a re ned comparison between policies derived from different Markov Decision Processes ( MDP ) models .
We applied this methodology to a prior experiment where the goal was to select the best features to include in the MDP statespace .
Our results show that while some of the policies developed in the prior work exhibited very large confidence intervals , the policy developed from the best feature set had a much smaller confidence interval and thus showed very high reliability .


